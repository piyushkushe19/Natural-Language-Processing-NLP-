# -*- coding: utf-8 -*-
"""NLP Tokenization,Stemming & Lemmatization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m75gIFGaqnQuB_maJ7igQk4lJKGh30yg
"""

!pip install kaggle

from google.colab import files
files.upload()

import os
os.environ['KAGGLE_CONFIG_DIR'] = "/content"

!kaggle datasets download -d nalgiriyewithana/mcdonalds-store-reviews

import pandas as pd

# Load the McDonald's Store Reviews dataset
df = pd.read_csv('/content/McDonald_s_Reviews.csv', encoding='latin-1')

# Display the first few rows to check the structure
display(df.head())

# Check the column names
print(df.columns)

import pandas as pd
import nltk
from nltk.corpus import stopwords
import re # Import the regular expression module

# Download NLTK resources (if not done previously)
nltk.download('stopwords')

# Function for Tokenization using regex
def tokenize_text_regex(text):
    # Ensure text is a string before tokenizing
    if isinstance(text, str):
        # Use regex to find words (sequences of letters)
        return re.findall(r'\b\w+\b', text.lower()) # Convert to lower case and find word boundaries
    else:
        return [] # Return empty list for non-string input

# Function for Stop Word Removal
def remove_stop_words(tokens):
    stop_words = set(stopwords.words('english'))
    return [word for word in tokens if word not in stop_words]

# Load the McDonald's Store Reviews dataset
# The file was successfully loaded in a previous cell, so we can use the existing DataFrame 'df'
# df = pd.read_csv('/content/McDonald_s_Reviews.csv', encoding='latin-1')

# Check the first few rows to identify the column name containing reviews
print(df.head())

# Assuming the column with reviews is named 'review'
df['tokens'] = df['review'].apply(tokenize_text_regex) # Use the regex tokenization function
df['filtered_tokens'] = df['tokens'].apply(remove_stop_words)

# Display the tokenized and filtered reviews
print(df[['review', 'tokens', 'filtered_tokens']].head())

!pip install nltk

import nltk
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
import pandas as pd

# Download required NLTK resources
nltk.download('wordnet')
nltk.download('punkt')

import nltk
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet # Import wordnet to map POS tags
import pandas as pd

# Download required NLTK resources
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger') # Download the POS tagger

# Initialize the stemmer and lemmatizer
stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

# Function to convert NLTK POS tags to WordNet POS tags
def get_wordnet_pos(tag):
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN # Default to noun

# Function for Stemming
def apply_stemming(tokens):
    return [stemmer.stem(word) for word in tokens]

# Function for Lemmatization
def apply_lemmatization(tokens):
    lemmatized_tokens = []
    # Get POS tags for the tokens
    pos_tags = nltk.pos_tag(tokens)
    for word, tag in pos_tags:
        # Convert NLTK tag to WordNet tag
        w_pos = get_wordnet_pos(tag)
        lemmatized_tokens.append(lemmatizer.lemmatize(word, pos=w_pos))
    return lemmatized_tokens


# Load the McDonald's Store Reviews dataset
# df = pd.read_csv('/content/McDonald_s_Reviews.csv', encoding='latin-1') # We already have df loaded

# Apply stemming and lemmatization to the filtered tokens
df['stemmed_tokens'] = df['filtered_tokens'].apply(apply_stemming)
df['lemmatized_tokens'] = df['filtered_tokens'].apply(apply_lemmatization)

# Display the original, stemmed, and lemmatized reviews
# Assuming the review column is named 'review' (adjust if necessary)
print(df[['review', 'tokens', 'filtered_tokens', 'stemmed_tokens', 'lemmatized_tokens']].head())